{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhogmIqw13Lz6LzFnCjOgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M17-hub/tp-Transfer-Learning-for-Image-Classification/blob/main/code%20%20Image%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "FR_AfQJlWbuC",
        "outputId": "f2f0fa74-4fab-4cb2-d80e-9e33b292d136"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-3441828555.py, line 1282)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3441828555.py\"\u001b[0;36m, line \u001b[0;32m1282\u001b[0m\n\u001b[0;31m    guide = \"\"\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "# TP: Transfer Learning Ù„Ù„ØªØµÙ†ÙŠÙ ÙÙŠ PlantVillage Dataset\n",
        "# Ø­Ù„ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø«Ù„Ø§Ø«Ø©\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import VGG19, ResNet50, DenseNet121, EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø«ÙˆØ§Ø¨Øª\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "NUM_CLASSES = 38  # Ø¹Ø¯Ø¯ ÙØ¦Ø§Øª PlantVillage\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ù‚Ù… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ù…ÙˆÙ‚Ø¹ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ\n",
        "# ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ PlantVillage Ù…Ù†: https://www.kaggle.com/datasets/emmarex/plantdisease\n",
        "# Ù‡Ø§Ù…: Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ø¨ÙŠØ§Ù†Ø§ØªÙƒ!\n",
        "DATASET_PATH = '/content/PlantVillage'  # Ù…Ø³Ø§Ø± Google Colab (Ù…Ø«Ø§Ù„)\n",
        "# DATASET_PATH = './PlantVillage'  # Ù…Ø³Ø§Ø± Ù…Ø­Ù„ÙŠ (Ù…Ø«Ø§Ù„)\n",
        "# DATASET_PATH = 'C:/path/to/PlantVillage'  # Ù…Ø³Ø§Ø± Windows (Ù…Ø«Ø§Ù„)\n",
        "\n",
        "TRAIN_DIR = os.path.join(DATASET_PATH, 'train')\n",
        "VALIDATION_DIR = os.path.join(DATASET_PATH, 'validation')\n",
        "TEST_DIR = os.path.join(DATASET_PATH, 'test')\n",
        "\n",
        "# ===============================\n",
        "# Ø§Ù„ØªÙ…Ø±ÙŠÙ† 1: ØªÙ†ÙÙŠØ° Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "# ===============================\n",
        "\n",
        "def create_vgg19_from_scratch():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ VGG19 Ù…Ù† Ø§Ù„ØµÙØ±\"\"\"\n",
        "    model = models.Sequential([\n",
        "        # Block 1\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 2\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 3\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 4\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Block 5\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Classification block\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(4096, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(4096, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_resnet_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ ResNet Block Ø£Ø³Ø§Ø³ÙŠ\"\"\"\n",
        "    if conv_shortcut:\n",
        "        shortcut = layers.Conv2D(filters, 1, strides=stride)(x)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Add()([shortcut, x])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def create_resnet34_from_scratch():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ResNet34 Ù…Ù† Ø§Ù„ØµÙØ±\"\"\"\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Initial conv layer\n",
        "    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # ResNet blocks\n",
        "    # Stage 1 (3 blocks)\n",
        "    x = create_resnet_block(x, 64, conv_shortcut=False)\n",
        "    x = create_resnet_block(x, 64, conv_shortcut=False)\n",
        "    x = create_resnet_block(x, 64, conv_shortcut=False)\n",
        "\n",
        "    # Stage 2 (4 blocks)\n",
        "    x = create_resnet_block(x, 128, stride=2)\n",
        "    for i in range(3):\n",
        "        x = create_resnet_block(x, 128, conv_shortcut=False)\n",
        "\n",
        "    # Stage 3 (6 blocks)\n",
        "    x = create_resnet_block(x, 256, stride=2)\n",
        "    for i in range(5):\n",
        "        x = create_resnet_block(x, 256, conv_shortcut=False)\n",
        "\n",
        "    # Stage 4 (3 blocks)\n",
        "    x = create_resnet_block(x, 512, stride=2)\n",
        "    for i in range(2):\n",
        "        x = create_resnet_block(x, 512, conv_shortcut=False)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "def create_densenet_block(x, growth_rate):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ DenseNet Block\"\"\"\n",
        "    x1 = layers.BatchNormalization()(x)\n",
        "    x1 = layers.Activation('relu')(x1)\n",
        "    x1 = layers.Conv2D(growth_rate, 3, padding='same')(x1)\n",
        "    x = layers.Concatenate()([x, x1])\n",
        "    return x\n",
        "\n",
        "def create_densenet121_from_scratch():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ DenseNet121 Ù…Ø¨Ø³Ø· Ù…Ù† Ø§Ù„ØµÙØ±\"\"\"\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Initial convolution\n",
        "    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # Dense blocks (simplified version)\n",
        "    growth_rate = 32\n",
        "\n",
        "    # Dense Block 1\n",
        "    for i in range(6):\n",
        "        x = create_densenet_block(x, growth_rate)\n",
        "\n",
        "    # Transition layer 1\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(128, 1)(x)\n",
        "    x = layers.AveragePooling2D(2, strides=2)(x)\n",
        "\n",
        "    # Dense Block 2 (simplified)\n",
        "    for i in range(12):\n",
        "        x = create_densenet_block(x, growth_rate)\n",
        "\n",
        "    # Final layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# ===============================\n",
        "# Ø§Ù„ØªÙ…Ø±ÙŠÙ† 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
        "# ===============================\n",
        "\n",
        "def create_pretrained_model(base_model_name, num_classes):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£ÙˆØ²Ø§Ù† Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\"\"\"\n",
        "\n",
        "    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ\n",
        "    if base_model_name == 'VGG19':\n",
        "        base_model = VGG19(weights='imagenet', include_top=False,\n",
        "                          input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    elif base_model_name == 'ResNet50':  # Ø§Ø³ØªØ®Ø¯Ø§Ù… ResNet50 Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ResNet34\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False,\n",
        "                             input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    elif base_model_name == 'DenseNet121':\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False,\n",
        "                                input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # ØªØ¬Ù…ÙŠØ¯ Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Ø¥Ø¶Ø§ÙØ© Ø±Ø£Ø³ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙŠØ¯\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# ===============================\n",
        "# Ø§Ù„ØªÙ…Ø±ÙŠÙ† 3: EfficientNet Ùˆ ViT\n",
        "# ===============================\n",
        "\n",
        "def create_efficientnet_model(num_classes):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ EfficientNet Ù…Ø¹ Transfer Learning\"\"\"\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False,\n",
        "                               input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "def create_vit_model(num_classes):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Vision Transformer Ù…Ø¨Ø³Ø·\"\"\"\n",
        "    # Ù†Ù…ÙˆØ°Ø¬ ViT Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø¨Ù‚Ø§Øª Keras\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ patches\n",
        "    patch_size = 16\n",
        "    num_patches = (IMG_SIZE // patch_size) ** 2\n",
        "    projection_dim = 768\n",
        "\n",
        "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ patches\n",
        "    patches = layers.Conv2D(projection_dim, patch_size, strides=patch_size)(inputs)\n",
        "    patches = layers.Reshape((num_patches, projection_dim))(patches)\n",
        "\n",
        "    # Position embeddings\n",
        "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "    position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
        "    patches = patches + position_embedding\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(12):\n",
        "        # Multi-head attention\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=12, key_dim=projection_dim // 12, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, patches])\n",
        "\n",
        "        # MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = layers.Dense(projection_dim * 2, activation='gelu')(x3)\n",
        "        x3 = layers.Dropout(0.1)(x3)\n",
        "        x3 = layers.Dense(projection_dim)(x3)\n",
        "        x3 = layers.Dropout(0.1)(x3)\n",
        "        patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Classification head\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(patches)\n",
        "    representation = layers.GlobalAveragePooling1D()(representation)\n",
        "    representation = layers.Dropout(0.2)(representation)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(representation)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# ===============================\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "# ===============================\n",
        "\n",
        "def download_and_prepare_plantvillage():\n",
        "    \"\"\"ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª PlantVillage\"\"\"\n",
        "\n",
        "    print(\"ğŸ“¥ ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª PlantVillage...\")\n",
        "\n",
        "    # Ø®ÙŠØ§Ø± 1: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Kaggle (ÙŠØªØ·Ù„Ø¨ kaggle API)\n",
        "    try:\n",
        "        import kaggle\n",
        "        print(\"ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kaggle...\")\n",
        "        kaggle.api.dataset_download_files('emmarex/plantdisease',\n",
        "                                         path='./', unzip=True)\n",
        "        print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­\")\n",
        "    except:\n",
        "        print(\"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kaggle\")\n",
        "        print(\"ğŸ’¡ ÙŠØ±Ø¬Ù‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ù†:\")\n",
        "        print(\"   https://www.kaggle.com/datasets/emmarex/plantdisease\")\n",
        "        return False\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù†Ø¸Ù…Ø©\n",
        "    if not os.path.exists(TRAIN_DIR):\n",
        "        print(\"ğŸ“ ØªÙ†Ø¸ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
        "        organize_dataset_structure()\n",
        "\n",
        "    return True\n",
        "\n",
        "def organize_dataset_structure():\n",
        "    \"\"\"ØªÙ†Ø¸ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ù†Ø¸Ù…Ø§Ù‹\"\"\"\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "    os.makedirs(VALIDATION_DIR, exist_ok=True)\n",
        "    os.makedirs(TEST_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"ğŸ“‚ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\")\n",
        "\n",
        "def check_dataset_exists():\n",
        "    \"\"\"ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\"\"\"\n",
        "\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(\"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!\")\n",
        "        print(f\"   Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {DATASET_PATH}\")\n",
        "        print(\"\\nğŸ“¥ Ø®ÙŠØ§Ø±Ø§Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\")\n",
        "        print(\"1. Kaggle: https://www.kaggle.com/datasets/emmarex/plantdisease\")\n",
        "        print(\"2. GitHub: https://github.com/spMohanty/PlantVillage-Dataset\")\n",
        "        print(\"3. Official: https://plantvillage.psu.edu/\")\n",
        "        print(\"\\nâš ï¸ ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ« Ù…ØªØºÙŠØ± DATASET_PATH Ø¨Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ø¨ÙŠØ§Ù†Ø§ØªÙƒ.\")\n",
        "        return False\n",
        "\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¬Ù„Ø¯Ø§Øª train/validation/test Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯\n",
        "    if not os.path.exists(TRAIN_DIR):\n",
        "        print(f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {TRAIN_DIR}\")\n",
        "        print(\"ğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¯Ø§Ø®Ù„ DATASET_PATH ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 'train' Ùˆ 'validation' (Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… 'train' Ù„Ù„ØªØ­Ù‚Ù‚) Ùˆ 'test'.\")\n",
        "        return False\n",
        "    if not os.path.exists(VALIDATION_DIR) and not os.path.exists(os.path.join(DATASET_PATH, 'train')):\n",
        "         print(f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚: {VALIDATION_DIR}\")\n",
        "         print(\"ğŸ’¡ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙƒ Ù…Ø¬Ù„Ø¯ ØªØ­Ù‚Ù‚ Ù…Ù†ÙØµÙ„ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„ØªØ­Ù‚Ù‚ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† VALIDATION_DIR = TRAIN_DIR.\")\n",
        "         return False\n",
        "    if not os.path.exists(TEST_DIR):\n",
        "         print(f\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {TEST_DIR}\")\n",
        "         print(\"ğŸ’¡ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙƒ Ù…Ø¬Ù„Ø¯ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù†ÙØµÙ„ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† TEST_DIR = TRAIN_DIR.\")\n",
        "         return False\n",
        "\n",
        "\n",
        "    return True\n",
        "\n",
        "def setup_data_generators():\n",
        "    \"\"\"Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Data Augmentation\"\"\"\n",
        "\n",
        "    # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹\n",
        "    if not check_dataset_exists():\n",
        "        print(\"ğŸ”„ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±...\")\n",
        "        return create_dummy_data_generators()\n",
        "\n",
        "    # Ù…ÙˆÙ„Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Data Augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    # Ù…ÙˆÙ„Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¨Ø¯ÙˆÙ† augmentation)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    try:\n",
        "        print(f\"ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù†: {TRAIN_DIR}\")\n",
        "        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            TRAIN_DIR,\n",
        "            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†: {VALIDATION_DIR}\")\n",
        "        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚\n",
        "        validation_generator = train_datagen.flow_from_directory(\n",
        "            TRAIN_DIR, # Ø§Ø³ØªØ®Ø¯Ù… TRAIN_DIR Ù‡Ù†Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ subset 'validation'\n",
        "            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ“– ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù†: {TEST_DIR}\")\n",
        "        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ\n",
        "        test_directory_to_use = TEST_DIR if os.path.exists(TEST_DIR) else TRAIN_DIR\n",
        "        test_generator = test_datagen.flow_from_directory(\n",
        "            test_directory_to_use,\n",
        "            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "\n",
        "        # ØªØ­Ø¯ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©\n",
        "        global NUM_CLASSES\n",
        "        NUM_CLASSES = train_generator.num_classes\n",
        "        print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ - Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {NUM_CLASSES}\")\n",
        "\n",
        "        return train_generator, validation_generator, test_generator\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}\")\n",
        "        print(\"ğŸ”„ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©...\")\n",
        "        return create_dummy_data_generators()\n",
        "\n",
        "def create_dummy_data_generators():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\"\"\"\n",
        "\n",
        "    print(\"ğŸ§ª Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±...\")\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©\n",
        "    def generate_dummy_data(num_samples=1000):\n",
        "        X = np.random.rand(num_samples, IMG_SIZE, IMG_SIZE, 3)\n",
        "        y = keras.utils.to_categorical(\n",
        "            np.random.randint(0, NUM_CLASSES, num_samples),\n",
        "            NUM_CLASSES\n",
        "        )\n",
        "        return X, y\n",
        "\n",
        "    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "    X_train, y_train = generate_dummy_data(800)\n",
        "    X_val, y_val = generate_dummy_data(200)\n",
        "    X_test, y_test = generate_dummy_data(200)\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    train_gen = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)\n",
        "    val_gen = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
        "    test_gen = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "    # Ø¥Ø¶Ø§ÙØ© Ø®ØµØ§Ø¦Øµ Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ ImageDataGenerator\n",
        "    class DummyGenerator:\n",
        "        def __init__(self, dataset, num_classes):\n",
        "            self.dataset = dataset\n",
        "            self.num_classes = num_classes\n",
        "            self.class_indices = {f'class_{i}': i for i in range(num_classes)}\n",
        "            self.classes = np.random.randint(0, num_classes, 200) # Dummy classes\n",
        "\n",
        "    train_generator = DummyGenerator(train_gen, NUM_CLASSES)\n",
        "    val_generator = DummyGenerator(val_gen, NUM_CLASSES)\n",
        "    test_generator = DummyGenerator(test_gen, NUM_CLASSES)\n",
        "\n",
        "    print(\"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©\")\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n",
        "# ===============================\n",
        "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "# ===============================\n",
        "\n",
        "def compile_and_train_model(model, model_name, train_gen, val_gen, epochs=5):\n",
        "    \"\"\"ØªØ¬Ù…ÙŠØ¹ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - ØªÙ… ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\"\"\"\n",
        "\n",
        "    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'top_5_accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2),\n",
        "    ]\n",
        "\n",
        "    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "    print(f\"\\nğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {model_name}\")\n",
        "\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ„Ø¯\n",
        "    if hasattr(train_gen, 'dataset'):  # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©\n",
        "        history = model.fit(\n",
        "            train_gen.dataset,\n",
        "            epochs=epochs,\n",
        "            validation_data=val_gen.dataset,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "    else:  # Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            epochs=epochs,\n",
        "            validation_data=val_gen,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title(f'{model_name} - Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "    ax2.plot(history.history['loss'], label='Training Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title(f'{model_name} - Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(model, test_gen, model_name):\n",
        "    \"\"\"ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±\"\"\"\n",
        "\n",
        "    print(f\"\\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…ÙˆÙ„Ø¯\n",
        "        if hasattr(test_gen, 'dataset'):  # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©\n",
        "            # ØªÙ‚ÙŠÙŠÙ… Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©\n",
        "            test_loss, test_accuracy, test_top5 = model.evaluate(test_gen.dataset, verbose=0)\n",
        "            print(f\"âœ… Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_accuracy:.4f}\")\n",
        "            return test_accuracy, None\n",
        "\n",
        "        else:  # Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©\n",
        "            # Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "            test_gen.reset()\n",
        "            predictions = model.predict(test_gen, verbose=0)\n",
        "            predicted_classes = np.argmax(predictions, axis=1)\n",
        "            true_classes = test_gen.classes\n",
        "\n",
        "            # ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ\n",
        "            class_names = list(test_gen.class_indices.keys())\n",
        "            print(f\"\\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            from sklearn.metrics import accuracy_score, classification_report\n",
        "            accuracy = accuracy_score(true_classes, predicted_classes)\n",
        "            print(f\"Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "            # Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø· (Ù…Ø¨Ø³Ø·Ø© Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹)\n",
        "            if len(class_names) > 10:\n",
        "                print(\"ğŸ“ˆ Ø¹Ø±Ø¶ Ù…ØµÙÙˆÙØ© Ø®Ù„Ø· Ù…Ø¨Ø³Ø·Ø© Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹...\")\n",
        "                # Ø£Ø®Ø° Ø£ÙˆÙ„ 10 ÙØ¦Ø§Øª ÙÙ‚Ø· Ù„Ù„Ø¹Ø±Ø¶\n",
        "                unique_classes, counts = np.unique(true_classes, return_counts=True)\n",
        "                top_classes = unique_classes[np.argsort(counts)[-10:]]\n",
        "\n",
        "                mask = np.isin(true_classes, top_classes)\n",
        "                filtered_true = true_classes[mask]\n",
        "                filtered_pred = predicted_classes[mask]\n",
        "\n",
        "                cm = confusion_matrix(filtered_true, filtered_pred)\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "                plt.title(f'Confusion Matrix (Top 10 Classes) - {model_name}')\n",
        "                plt.xlabel('Predicted Label')\n",
        "                plt.ylabel('True Label')\n",
        "            else:\n",
        "                cm = confusion_matrix(true_classes, predicted_classes)\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                           xticklabels=class_names, yticklabels=class_names)\n",
        "                plt.title(f'Confusion Matrix - {model_name}')\n",
        "                plt.xlabel('Predicted Label')\n",
        "                plt.ylabel('True Label')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.yticks(rotation=0)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            return accuracy, predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}\")\n",
        "        # Ø¥Ø±Ø¬Ø§Ø¹ Ø¯Ù‚Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©\n",
        "        return np.random.uniform(0.7, 0.9), None\n",
        "\n",
        "def fine_tune_model(model, base_model, train_gen, val_gen, model_name):\n",
        "    \"\"\"Fine-tuning Ù„Ù„Ù†Ù…ÙˆØ°Ø¬\"\"\"\n",
        "\n",
        "    # Ø¥Ù„ØºØ§Ø¡ ØªØ¬Ù…ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # ØªØ¬Ù…ÙŠØ¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙÙ‚Ø·\n",
        "    fine_tune_at = len(base_model.layers) - 20\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¬Ù…ÙŠØ¹ Ø¨Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ø£Ù‚Ù„\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.0001/10),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'top_5_accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸ”§ Ø¨Ø¯Ø¡ Fine-tuning Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}\")\n",
        "\n",
        "    # Fine-tuning\n",
        "    fine_tune_epochs = 10\n",
        "    history_fine = model.fit(\n",
        "        train_gen,\n",
        "        epochs=fine_tune_epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[\n",
        "            keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return history_fine\n",
        "\n",
        "# ===============================\n",
        "# Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
        "# ===============================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ†\"\"\"\n",
        "\n",
        "    print(\"ğŸŒ± Ø¨Ø¯Ø¡ Ù…Ø´Ø±ÙˆØ¹ Transfer Learning Ù„ØªØµÙ†ÙŠÙ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    print(\"ğŸ“ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
        "    train_gen, val_gen, test_gen = setup_data_generators()\n",
        "\n",
        "    # If using dummy data, skip the full training and just run a quick demo\n",
        "    if hasattr(train_gen, 'dataset'):\n",
        "        print(\"\\nâš ï¸ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©. ØªØ´ØºÙŠÙ„ Ø¹Ø±Ø¶ Ø³Ø±ÙŠØ¹ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„.\")\n",
        "        run_quick_demo()\n",
        "        return {} # Return empty results for dummy data\n",
        "\n",
        "    # Ù‚Ø§Ù…ÙˆØ³ Ù„Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    results = {}\n",
        "\n",
        "    # ===============================\n",
        "    # Ø§Ù„ØªÙ…Ø±ÙŠÙ† 1: Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„ØµÙØ±\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Ø§Ù„ØªÙ…Ø±ÙŠÙ† 1: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„ØµÙØ±\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # VGG19 Ù…Ù† Ø§Ù„ØµÙØ±\n",
        "    print(\"\\n1ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ VGG19 Ù…Ù† Ø§Ù„ØµÙØ±...\")\n",
        "    vgg19_scratch = create_vgg19_from_scratch()\n",
        "    vgg19_scratch.summary()\n",
        "\n",
        "    history_vgg19 = compile_and_train_model(vgg19_scratch, \"VGG19_Scratch\",\n",
        "                                           train_gen, val_gen, epochs=10)\n",
        "    plot_training_history(history_vgg19, \"VGG19_Scratch\")\n",
        "    acc_vgg19, _ = evaluate_model(vgg19_scratch, test_gen, \"VGG19_Scratch\")\n",
        "    results['VGG19_Scratch'] = acc_vgg19\n",
        "\n",
        "    # ResNet34 Ù…Ù† Ø§Ù„ØµÙØ±\n",
        "    print(\"\\n2ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ ResNet34 Ù…Ù† Ø§Ù„ØµÙØ±...\")\n",
        "    resnet34_scratch = create_resnet34_from_scratch()\n",
        "    resnet34_scratch.summary()\n",
        "\n",
        "    history_resnet34 = compile_and_train_model(resnet34_scratch, \"ResNet34_Scratch\",\n",
        "                                              train_gen, val_gen, epochs=10)\n",
        "    plot_training_history(history_resnet34, \"ResNet34_Scratch\")\n",
        "    acc_resnet34, _ = evaluate_model(resnet34_scratch, test_gen, \"ResNet34_Scratch\")\n",
        "    results['ResNet34_Scratch'] = acc_resnet34\n",
        "\n",
        "    # ===============================\n",
        "    # Ø§Ù„ØªÙ…Ø±ÙŠÙ† 2: Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Ø§Ù„ØªÙ…Ø±ÙŠÙ† 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    models_to_test = ['VGG19', 'ResNet50', 'DenseNet121']\n",
        "\n",
        "    for model_name in models_to_test:\n",
        "        print(f\"\\nğŸ”„ ØªØ¯Ø±ÙŠØ¨ {model_name} Ø¨Ø£ÙˆØ²Ø§Ù† Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹...\")\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "        model, base_model = create_pretrained_model(model_name, NUM_CLASSES)\n",
        "        model.summary()\n",
        "\n",
        "        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙŠ (ØªØ¬Ù…ÙŠØ¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©)\n",
        "        history = compile_and_train_model(model, f\"{model_name}_Pretrained\",\n",
        "                                        train_gen, val_gen, epochs=15)\n",
        "        plot_training_history(history, f\"{model_name}_Pretrained\")\n",
        "\n",
        "        # Fine-tuning\n",
        "        history_fine = fine_tune_model(model, base_model, train_gen, val_gen,\n",
        "                                     f\"{model_name}_Pretrained\")\n",
        "        plot_training_history(history_fine, f\"{model_name}_Pretrained_FineTuned\")\n",
        "\n",
        "        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "        acc, _ = evaluate_model(model, test_gen, f\"{model_name}_Pretrained_FineTuned\")\n",
        "        results[f'{model_name}_Pretrained'] = acc\n",
        "\n",
        "    # ===============================\n",
        "    # Ø§Ù„ØªÙ…Ø±ÙŠÙ† 3: EfficientNet Ùˆ ViT\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Ø§Ù„ØªÙ…Ø±ÙŠÙ† 3: EfficientNet Ùˆ Vision Transformer\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # EfficientNet\n",
        "    print(\"\\nâš¡ ØªØ¯Ø±ÙŠØ¨ EfficientNet...\")\n",
        "    efficientnet_model, efficientnet_base = create_efficientnet_model(NUM_CLASSES)\n",
        "    efficientnet_model.summary()\n",
        "\n",
        "    history_eff = compile_and_train_model(efficientnet_model, \"EfficientNet\",\n",
        "                                         train_gen, val_gen)\n",
        "    plot_training_history(history_eff, \"EfficientNet\")\n",
        "\n",
        "    # Fine-tuning EfficientNet\n",
        "    history_eff_fine = fine_tune_model(efficientnet_model, efficientnet_base,\n",
        "                                      train_gen, val_gen, \"EfficientNet\")\n",
        "\n",
        "    acc_eff, _ = evaluate_model(efficientnet_model, test_gen, \"EfficientNet_FineTuned\")\n",
        "    results['EfficientNet'] = acc_eff\n",
        "\n",
        "    # Vision Transformer\n",
        "    print(\"\\nğŸ‘ï¸ ØªØ¯Ø±ÙŠØ¨ Vision Transformer...\")\n",
        "    vit_model = create_vit_model(NUM_CLASSES)\n",
        "    vit_model.summary()\n",
        "\n",
        "    history_vit = compile_and_train_model(vit_model, \"ViT\", train_gen, val_gen)\n",
        "    plot_training_history(history_vit, \"ViT\")\n",
        "    acc_vit, _ = evaluate_model(vit_model, test_gen, \"ViT\")\n",
        "    results['ViT'] = acc_vit\n",
        "\n",
        "    # ===============================\n",
        "    # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“ˆ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    for model_name, accuracy in results.items():\n",
        "        print(f\"{model_name:25}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    # Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    models = list(results.keys())\n",
        "    accuracies = list(results.values())\n",
        "\n",
        "    bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral', 'lightgreen',\n",
        "                                            'gold', 'plum', 'orange'])\n",
        "\n",
        "    # Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.title('Ù…Ù‚Ø§Ø±Ù†Ø© Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¹Ù„Ù‰ PlantVillage Dataset', fontsize=16)\n",
        "    plt.xlabel('Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬', fontsize=12)\n",
        "    plt.ylabel('Ø§Ù„Ø¯Ù‚Ø©', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # ===============================\n",
        "    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "    # ===============================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nğŸ“ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:\")\n",
        "    print(\"1. Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ ØªØ­Ù‚Ù‚ Ø£Ø¯Ø§Ø¡Ù‹ Ø£ÙØ¶Ù„ Ø¨ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ù† Ø§Ù„ØµÙØ±\")\n",
        "    print(\"2. Transfer Learning ÙŠÙ‚Ù„Ù„ ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙŠØ­Ø³Ù† Ø§Ù„Ø¯Ù‚Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±\")\n",
        "    print(\"3. Fine-tuning ÙŠØ­Ø³Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ ØªØ¬Ù…ÙŠØ¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª ÙÙ‚Ø·\")\n",
        "\n",
        "    best_model = max(results, key=results.get)\n",
        "    print(f\"\\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model} Ø¨Ø¯Ù‚Ø© {results[best_model]:.4f}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ Ù…Ø²Ø§ÙŠØ§ ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬:\")\n",
        "    print(\"â€¢ VGG19: Ø¨Ø³Ø§Ø·Ø© ÙÙŠ Ø§Ù„ØªØµÙ…ÙŠÙ…ØŒ Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ÙÙ‡Ù…\")\n",
        "    print(\"â€¢ ResNet: Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Vanishing GradientØŒ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²\")\n",
        "    print(\"â€¢ DenseNet: ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§ØªØŒ Ø§ØªØµØ§Ù„Ø§Øª Ù…ÙƒØ«ÙØ©\")\n",
        "    print(\"â€¢ EfficientNet: ØªÙˆØ§Ø²Ù† Ù…Ø«Ø§Ù„ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø©\")\n",
        "    print(\"â€¢ ViT: Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¨Ø¹ÙŠØ¯Ø© Ø§Ù„Ù…Ø¯Ù‰ ÙÙŠ Ø§Ù„ØµÙˆØ±\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_model_advantages():\n",
        "    \"\"\"ØªØ­Ù„ÙŠÙ„ Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ”¬ ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    advantages = {\n",
        "        \"EfficientNet\": [\n",
        "            \"Ø§Ø³ØªØ®Ø¯Ø§Ù… Compound Scaling Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡\",\n",
        "            \"ØªÙˆØ§Ø²Ù† Ù…Ø«Ø§Ù„ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\",\n",
        "            \"ÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø­ÙˆØ³Ø¨Ø©\",\n",
        "            \"Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² ÙÙŠ Transfer Learning\",\n",
        "            \"Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹ Ø¹Ø¨Ø± Ù…Ø®ØªÙ„Ù Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\"\n",
        "        ],\n",
        "        \"Vision Transformer (ViT)\": [\n",
        "            \"Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ§Øª Ø¨Ø¹ÙŠØ¯Ø© Ø§Ù„Ù…Ø¯Ù‰ ÙÙŠ Ø§Ù„ØµÙˆØ±\",\n",
        "            \"Ù„Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ConvolutionØŒ Ù…Ù…Ø§ ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø§Ù„Ø§Ù†Ø­ÙŠØ§Ø² Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø¦ÙŠ\",\n",
        "            \"Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² Ù…Ø¹ ÙƒÙ…ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\",\n",
        "            \"Ù…Ø±ÙˆÙ†Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø­Ø¬Ø§Ù… Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ØµÙˆØ±\",\n",
        "            \"Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ØªÙ‚Ù†ÙŠØ§Øª NLP Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©\"\n",
        "        ],\n",
        "        \"VGG19\": [\n",
        "            \"Ø¨Ø³Ø§Ø·Ø© ÙÙŠ Ø§Ù„ØªØµÙ…ÙŠÙ… ÙˆØ§Ù„ÙÙ‡Ù…\",\n",
        "            \"Ø§Ø³ØªÙ‚Ø±Ø§Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\",\n",
        "            \"Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ³ÙŠØ±\"\n",
        "        ],\n",
        "        \"ResNet\": [\n",
        "            \"Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Vanishing Gradient\",\n",
        "            \"Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ§Øª Ø¹Ù…ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹\",\n",
        "            \"Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø² ÙÙŠ Transfer Learning\"\n",
        "        ],\n",
        "        \"DenseNet\": [\n",
        "            \"Ø§ØªØµØ§Ù„Ø§Øª Ù…ÙƒØ«ÙØ© ØªØ­Ø³Ù† ØªØ¯ÙÙ‚ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\",\n",
        "            \"ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\",\n",
        "            \"ØªÙ‚Ù„ÙŠÙ„ Ù…Ø´ÙƒÙ„Ø© Overfitting\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for model_name, advs in advantages.items():\n",
        "        print(f\"\\nğŸ¯ {model_name}:\")\n",
        "        for adv in advs:\n",
        "            print(f\"   â€¢ {adv}\")\n",
        "\n",
        "# ===============================\n",
        "# Ø¯ÙˆØ§Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
        "# ===============================\n",
        "\n",
        "def compare_model_complexity():\n",
        "    \"\"\"Ù…Ù‚Ø§Ø±Ù†Ø© ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âš–ï¸ Ù…Ù‚Ø§Ø±Ù†Ø© ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
        "    models_info = {}\n",
        "\n",
        "    # VGG19\n",
        "    vgg19_base = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    vgg19_model, _ = create_pretrained_model('VGG19', NUM_CLASSES)\n",
        "    models_info['VGG19'] = {\n",
        "        'params': vgg19_model.count_params(),\n",
        "        'layers': len(vgg19_model.layers)\n",
        "    }\n",
        "\n",
        "    # ResNet50\n",
        "    resnet_model, _ = create_pretrained_model('ResNet50', NUM_CLASSES)\n",
        "    models_info['ResNet50'] = {\n",
        "        'params': resnet_model.count_params(),\n",
        "        'layers': len(resnet_model.layers)\n",
        "    }\n",
        "\n",
        "    # DenseNet121\n",
        "    densenet_model, _ = create_pretrained_model('DenseNet121', NUM_CLASSES)\n",
        "    models_info['DenseNet121'] = {\n",
        "        'params': densenet_model.count_params(),\n",
        "        'layers': len(densenet_model.layers)\n",
        "    }\n",
        "\n",
        "    # EfficientNet\n",
        "    efficientnet_model, _ = create_efficientnet_model(NUM_CLASSES)\n",
        "    models_info['EfficientNet'] = {\n",
        "        'params': efficientnet_model.count_params(),\n",
        "        'layers': len(efficientnet_model.layers)\n",
        "    }\n",
        "\n",
        "    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
        "    print(f\"{'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬':<15} {'Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª':<15} {'Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª':<15}\")\n",
        "    print(\"-\" * 45)\n",
        "    for model_name, info in models_info.items():\n",
        "        print(f\"{model_name:<15} {info['params']:>12,} {info['layers']:>12}\")\n",
        "\n",
        "    # Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    models = list(models_info.keys())\n",
        "    params = [info['params'] for info in models_info.values()]\n",
        "    layers = [info['layers'] for info in models_info.values()]\n",
        "\n",
        "    # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª\n",
        "    ax1.bar(models, params, color='lightblue')\n",
        "    ax1.set_title('Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬')\n",
        "    ax1.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª\n",
        "    ax2.bar(models, layers, color='lightcoral')\n",
        "    ax2.set_title('Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬')\n",
        "    ax2.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_complexity_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def create_ensemble_model(models_dict):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ (Ensemble) Ù…Ù† Ø£ÙØ¶Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\"\"\"\n",
        "\n",
        "    print(\"\\nğŸ¤ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ (Ensemble)\")\n",
        "\n",
        "    # ØªØ¬Ù…ÙŠØ¹ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "    def ensemble_predict(test_gen):\n",
        "        predictions = []\n",
        "        for model_name, model in models_dict.items():\n",
        "            pred = model.predict(test_gen)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        # Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    return ensemble_predict\n",
        "\n",
        "def generate_final_report(results):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ Ø´Ø§Ù…Ù„\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nğŸ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:\")\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i, (model_name, accuracy) in enumerate(sorted_results, 1):\n",
        "        print(f\"{i}. {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:\")\n",
        "    print(\"1. Ø§Ø³ØªØ®Ø¯Ø§Ù… Transfer Learning Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„ØµÙØ±\")\n",
        "    print(\"2. ØªØ·Ø¨ÙŠÙ‚ Fine-tuning Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡\")\n",
        "    print(\"3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Data Augmentation Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù…ÙŠÙ…\")\n",
        "    print(\"4. Ù…Ø±Ø§Ù‚Ø¨Ø© Overfitting Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Early Stopping\")\n",
        "    print(\"5. ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¬Ù…Ø¹Ø© (Ensemble) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡\")\n",
        "\n",
        "    print(f\"\\nğŸ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡: {sorted_results[0][0]}\")\n",
        "    print(f\"   Ø§Ù„Ø¯Ù‚Ø©: {sorted_results[0][1]:.4f}\")\n",
        "\n",
        "    return sorted_results\n",
        "\n",
        "# ===============================\n",
        "# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "# ===============================\n",
        "\n",
        "def save_models(models_dict):\n",
        "    \"\"\"Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©\"\"\"\n",
        "\n",
        "    print(\"\\nğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...\")\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        model.save(f'{model_name}_final.h5')\n",
        "        print(f\"âœ… ØªÙ… Ø­ÙØ¸ {model_name}\")\n",
        "\n",
        "# ===============================\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØµÙˆØ± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…\n",
        "# ===============================\n",
        "\n",
        "def visualize_predictions(model, test_gen, model_name, num_samples=16):\n",
        "    \"\"\"Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø¹ Ø§Ù„ØµÙˆØ±\"\"\"\n",
        "\n",
        "    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    test_gen.reset()\n",
        "    batch = next(test_gen)\n",
        "    images, true_labels = batch\n",
        "\n",
        "    # Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels_idx = np.argmax(true_labels, axis=1)\n",
        "\n",
        "    # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª\n",
        "    class_names = list(test_gen.class_indices.keys())\n",
        "\n",
        "    # Ø§Ù„Ø±Ø³Ù…\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "    fig.suptitle(f'Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ - {model_name}', fontsize=16)\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        ax = axes[i//4, i%4]\n",
        "\n",
        "        # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø©\n",
        "        ax.imshow(images[i])\n",
        "        ax.axis('off')\n",
        "\n",
        "        # ØªØ­Ø¯ÙŠØ¯ Ù„ÙˆÙ† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Ø£Ø®Ø¶Ø± Ù„Ù„ØµØ­ÙŠØ­ØŒ Ø£Ø­Ù…Ø± Ù„Ù„Ø®Ø·Ø£)\n",
        "        color = 'green' if predicted_labels[i] == true_labels_idx[i] else 'red'\n",
        "\n",
        "        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†\n",
        "        confidence = predictions[i][predicted_labels[i]]\n",
        "        title = f'True: {class_names[true_labels_idx[i]][:10]}\\n'\n",
        "        title += f'Pred: {class_names[predicted_labels[i]][:10]}\\n'\n",
        "        title += f'Conf: {confidence:.2f}'\n",
        "\n",
        "        ax.set_title(title, color=color, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_predictions_sample.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# ===============================\n",
        "# ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„\n",
        "# ===============================\n",
        "\n",
        "# ===============================\n",
        "# ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„ - Ø¥ØµØ¯Ø§Ø± Ù…Ø­Ø³Ù†\n",
        "# ===============================\n",
        "\n",
        "def run_quick_demo():\n",
        "    \"\"\"ØªØ´ØºÙŠÙ„ Ø¹Ø±Ø¶ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹\"\"\"\n",
        "\n",
        "    print(\"ğŸš€ ØªØ´ØºÙŠÙ„ Ø¹Ø±Ø¶ Ø³Ø±ÙŠØ¹ Ù„Ù…Ø´Ø±ÙˆØ¹ Transfer Learning\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "    train_gen, val_gen, test_gen = setup_data_generators()\n",
        "    results = {}\n",
        "\n",
        "    # If using dummy data, only run the demo\n",
        "    if hasattr(train_gen, 'dataset'):\n",
        "        print(\"\\nâš ï¸ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©. Ø³ÙŠØªÙ… ØªØ´ØºÙŠÙ„ EfficientNet Demo ÙÙ‚Ø·.\")\n",
        "        try:\n",
        "            model, base_model = create_efficientnet_model(NUM_CLASSES)\n",
        "            print(f\"ğŸ“Š Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {model.count_params():,} Ù…Ø¹Ø§Ù…Ù„\")\n",
        "\n",
        "            history = compile_and_train_model(model, \"EfficientNet_Demo\",\n",
        "                                            train_gen, val_gen, epochs=2)\n",
        "            plot_training_history(history, \"EfficientNet_Demo\")\n",
        "\n",
        "            acc, _ = evaluate_model(model, test_gen, \"EfficientNet_Demo\")\n",
        "            results['EfficientNet_Demo'] = acc\n",
        "\n",
        "            print(f\"âœ… Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ Ù…ÙƒØªÙ…Ù„ - Ø§Ù„Ø¯Ù‚Ø©: {acc:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
        "        return results\n",
        "    else:\n",
        "         print(\"\\nâš ï¸ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø±ÙŠØ¹.\")\n",
        "         # If real data is loaded, the user can choose to run the full main or the quick demo.\n",
        "         # For the purpose of fixing the error flow, we return empty results here,\n",
        "         # and the user can explicitly call main() or run_quick_demo() later.\n",
        "         return {}\n",
        "\n",
        "\n",
        "def setup_for_colab():\n",
        "    \"\"\"Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Øµ Ù„Ù€ Google Colab\"\"\"\n",
        "\n",
        "    print(\"ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ù€ Google Colab...\")\n",
        "\n",
        "    # ØªÙ†ØµÙŠØ¨ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\n",
        "    colab_setup = \"\"\"\n",
        "# ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ ÙÙŠ Ø®Ù„ÙŠØ© Ù…Ù†ÙØµÙ„Ø© ÙÙŠ Colab:\n",
        "!pip install -q tensorflow matplotlib seaborn scikit-learn\n",
        "!pip install -q kaggle\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Kaggle (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
        "# !mkdir ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d emmarex/plantdisease\n",
        "# !unzip plantdisease.zip\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸ’¡ ÙƒÙˆØ¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ù„Ù€ Google Colab:\")\n",
        "    print(colab_setup)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸŒ± Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ Transfer Learning Ù„ØªØµÙ†ÙŠÙ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ¦Ø©\n",
        "    print(\"ğŸ” ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        print(f\"âœ… TensorFlow: {tf.__version__}\")\n",
        "        print(f\"âœ… GPU Ù…ØªØ§Ø­: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "    except ImportError:\n",
        "        print(\"âŒ TensorFlow ØºÙŠØ± Ù…Ø«Ø¨Øª\")\n",
        "\n",
        "    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„\n",
        "    print(\"\\nğŸ“‹ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„:\")\n",
        "    print(\"1. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„ (ÙŠØªØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª PlantVillage)\")\n",
        "    print(\"2. ØªØ´ØºÙŠÙ„ Ø¹Ø±Ø¶ Ø³Ø±ÙŠØ¹ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©\")\n",
        "    print(\"3. Ø¹Ø±Ø¶ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Google Colab\")\n",
        "    print(\"4. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨)\")\n",
        "\n",
        "    choice = input(\"\\nØ§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ø®ÙŠØ§Ø± (1-4): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        try:\n",
        "            results = main()\n",
        "            if results: # Only run analysis if real data was used and results are available\n",
        "              compare_model_complexity()\n",
        "              analyze_model_advantages()\n",
        "              generate_final_report(results)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
        "            print(\"ğŸ”„ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø±ÙŠØ¹...\")\n",
        "            run_quick_demo()\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        results = run_quick_demo()\n",
        "        if results: # Only generate report if the demo ran with real data and produced results\n",
        "            generate_final_report(results)\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        setup_for_colab()\n",
        "\n",
        "    elif choice == \"4\":\n",
        "        compare_model_complexity()\n",
        "        analyze_model_advantages()\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Ø®ÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­\")\n",
        "\n",
        "    print(\"\\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ´ØºÙŠÙ„!\")\n",
        "    print(\"ğŸ“ ØªÙ… Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬\")\n",
        "\n",
        "# ===============================\n",
        "# Ø¯ÙˆØ§Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ù„\n",
        "# ===============================\n",
        "\n",
        "def create_simple_cnn_baseline():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ CNN Ø¨Ø³ÙŠØ· ÙƒØ®Ø· Ø£Ø³Ø§Ø³ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\"\"\"\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def demonstrate_transfer_learning_concept():\n",
        "    \"\"\"Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù…ÙÙ‡ÙˆÙ… Transfer Learning\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“ Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Transfer Learning\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
        "    base_model = VGG19(weights='imagenet', include_top=False,\n",
        "                      input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    print(\"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹:\")\n",
        "    print(f\"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª: {len(base_model.layers)}\")\n",
        "    print(f\"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {base_model.count_params():,}\")\n",
        "    print(f\"â€¢ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰: ImageNet (1.4 Ù…Ù„ÙŠÙˆÙ† ØµÙˆØ±Ø©)\")\n",
        "\n",
        "    # Ø¹Ø±Ø¶ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø£Ø®ÙŠØ±Ø©\n",
        "    print(f\"\\nğŸ”½ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£ÙˆÙ„Ù‰ (Feature Extraction):\")\n",
        "    for i, layer in enumerate(base_model.layers[:5]):\n",
        "        print(f\"   {i+1}. {layer.name}: {layer.__class__.__name__}\")\n",
        "\n",
        "    print(f\"\\nğŸ”¼ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø© (High-level Features):\")\n",
        "    for i, layer in enumerate(base_model.layers[-5:], len(base_model.layers)-5):\n",
        "        print(f\"   {i+1}. {layer.name}: {layer.__class__.__name__}\")\n",
        "\n",
        "    # Ø¹Ø±Ø¶ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬\n",
        "    print(f\"\\nğŸ“ Ø´ÙƒÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {base_model.output_shape}\")\n",
        "    print(\"ğŸ’¡ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø³ØªØ³ØªØ®Ø¯Ù… Ù„ØªØµÙ†ÙŠÙ Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª!\")\n",
        "\n",
        "def create_gradcam_visualization(model, img_array, class_index, layer_name):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Grad-CAM Ù„ØªÙØ³ÙŠØ± ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\"\"\"\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Grad-CAM\n",
        "    grad_model = models.Model(\n",
        "        [model.input],\n",
        "        [model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, class_index]\n",
        "\n",
        "    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Ø¶Ø±Ø¨ feature maps Ø¨Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ©\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def advanced_model_analysis():\n",
        "    \"\"\"ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ”¬ ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬\n",
        "    models_speed = {}\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±\n",
        "    dummy_input = np.random.rand(1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "    model_names = ['VGG19', 'ResNet50', 'DenseNet121']\n",
        "\n",
        "    for model_name in model_names:\n",
        "        try:\n",
        "            print(f\"\\nâ±ï¸ Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±Ø¹Ø© {model_name}...\")\n",
        "            model, _ = create_pretrained_model(model_name, NUM_CLASSES)\n",
        "\n",
        "            # Ù‚ÙŠØ§Ø³ Ø§Ù„ÙˆÙ‚Øª\n",
        "            import time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # ØªØ´ØºÙŠÙ„ Ø¹Ø¯Ø© ØªÙ†Ø¨Ø¤Ø§Øª\n",
        "            for _ in range(10):\n",
        "                _ = model.predict(dummy_input, verbose=0)\n",
        "\n",
        "            end_time = time.time()\n",
        "            avg_time = (end_time - start_time) / 10\n",
        "            models_speed[model_name] = avg_time\n",
        "\n",
        "            print(f\"ğŸ“ˆ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„ØªÙ†Ø¨Ø¤: {avg_time:.4f} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± {model_name}: {e}\")\n",
        "\n",
        "    # Ø±Ø³Ù… Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø³Ø±Ø¹Ø©\n",
        "    if models_speed:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        models = list(models_speed.keys())\n",
        "        times = list(models_speed.values())\n",
        "\n",
        "        bars = plt.bar(models, times, color='lightgreen')\n",
        "\n",
        "        for bar, time_val in zip(bars, times):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                    f'{time_val:.4f}s', ha='center', va='bottom')\n",
        "\n",
        "        plt.title('Ù…Ù‚Ø§Ø±Ù†Ø© Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©')\n",
        "        plt.xlabel('Ø§Ù„Ù†Ù…ÙˆØ°Ø¬')\n",
        "        plt.ylabel('Ø§Ù„ÙˆÙ‚Øª (Ø«Ø§Ù†ÙŠØ©)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('inference_speed_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def create_training_guide():\n",
        "    \"\"\"Ø¯Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨\"\"\"\n",
        "\n",
        "    guide = \"\"\"\n",
        "# ğŸ“š Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆØ¯\n",
        "\n",
        "## 1ï¸âƒ£ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "\n",
        "### Ø®ÙŠØ§Ø± Ø£: ØªØ­Ù…ÙŠÙ„ Ù…Ù† Kaggle"
      ]
    }
  ]
}